{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f877a539",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3faa010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7e940",
   "metadata": {},
   "source": [
    "## Patch Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572ca655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        size of the image (square image)\n",
    "        \n",
    "    patch_size : int\n",
    "        size of the patch (square)\n",
    "        \n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "        \n",
    "    embed_dim : int\n",
    "        The embedding dimension.\n",
    "        This remains constant across the network.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches we split our image into.\n",
    "        \n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer to split the image into patches.\n",
    "        It does two things: splitting as well as the embedding.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_sizeself.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) **2  # we'll try that the image size is perfectly divisible by the patch size so that we completely cover the image\n",
    "        \n",
    "        # the attribute projection\n",
    "        # we put both kernel_size and the stride as the patch_size\n",
    "        # this way, when we are sliding the kernel along the input tensor, we will never overlap the kernel\n",
    "        # the kernel will exactly fall into patches that we're trying to divide our image into\n",
    "        self.proj = nn.Conv2d(\n",
    "                    in_chans,\n",
    "                    embed_dim,\n",
    "                    kernel_size=patch_size,\n",
    "                    stride=patch_size)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            shape -> '(n_samples, in_chans, img_size, img_size)'\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            shape -> '(n_samples, n_patches, embed_dim)'\n",
    "            \n",
    "        \"\"\"\n",
    "        # The input tensor is nothing but a batch of images\n",
    "        # n_samples and batch size are synonymous\n",
    "        # we'll be using n_samples across the entire code\n",
    "        # because we're using pytorch, the channels are actually the 2nd dimension / 1st dimension in python terms\n",
    "        # image size is the height and the width of our images\n",
    "        # the otput is a three dimensional tensor \n",
    "        # the second dimension represents different patches that we divided into \n",
    "        # the last dimension will be the embedding dimension\n",
    "        \n",
    "        x = self.proj(x) # (n_smaples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        # we run the input tensor through the convlayer and we will get a 4D tensor\n",
    "        \n",
    "        x = x.flatten(2) # (n_samples, embed_dim, n_patches)\n",
    "        # we take the last two dimensions that represent the grid of patches and we flatten them into a single dimension\n",
    "        \n",
    "        x = x.transpose(1, 2) # (n_samples, n_patches, embed_dim)\n",
    "        # we finally swap two dimensions\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedeb81",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8596207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and output dimension of per token features\n",
    "        \n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "        \n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "        \n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "        \n",
    "    proj_p : float\n",
    "        Dropout probabilty applied to the output tensor.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing constant for the dot product\n",
    "        \n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "        \n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes the concatenated output of all attention heads and maps it into a new space.\n",
    "        \n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "    # we provide the embeding dimension and we will set things up in a way that our input dim of the tokens is equal to the output dimension.\n",
    "    # n_heads is another hyperparameter related to the attention mechanism\n",
    "    # qkv_bias will determine if we want to include the bias in the query, key and value proj\n",
    "    # we will be running the network only in the inference mode so we don't need any dropout\n",
    "    # but doupout helps with overfitting\n",
    "    \n",
    "    # internally we save a scale factor and it will be used to normalize the dot product\n",
    "    # the linear mapping can be split up into three seperate ones : key, value, query\n",
    "    # proj is another linear layer and is the last step of the attention mechanism\n",
    "    \n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        # here we define the dimensionality for each of the heads\n",
    "        # the reason for setting it up in this way is that once we concatenate all the attention heads, we'll get a new tensor that will have the same dimensionality as the input.\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        # the scale is coming from the attentionisallyouneed paper\n",
    "        # the idea behind it is not to feed extremely large values into the softmax which could lead into small gradients\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        # here we create a linear mapping that is going to take in a token embedding and generate a query, key and a value\n",
    "        # we can also write three seperate linear mappings that are more or less doing the same thing\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        # here we define two dropout layers \n",
    "        # define a linear mapping that takes the concatenated heads and maps them to a new space.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            shape -> '(n_samples, n_patches + 1, dim)'.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "           \n",
    "           shape -> '(n_samples, n_patches + 1, dim)'\n",
    "        \"\"\"\n",
    "        # in the forward pass, the input and the output tensors are going to have the same shape\n",
    "        # the second dimension is going to have a (size of the num patches + 1)\n",
    "        # reason of inlcuding the '+ 1' being : we willalways have the class token as the first token in the sequence.\n",
    "        \n",
    "        n_samples, n_tokens, dim = x. shape\n",
    "        \n",
    "        # just checking whether the embedding dimension of the input is the same as the one we declared in the constructor.\n",
    "        # SANITY CHECK\n",
    "        if dim !=self.dim:\n",
    "            raise ValueError\n",
    "            \n",
    "        # taking the input tensor and turning it into queries, keys and values\n",
    "        qkv = self.qkv(x)  # (n_samples, n_patches+1, 3*dim)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "                )  # (n_samples, n_patches+1, 2, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
    "        # in the reshape step we create an extra dimension for the heads and we also create an extra dimenion for the key, query and value\n",
    "        # in the permute step we just change their order\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, dead_dim, n_patches+1)\n",
    "        # here we transpose our keys because we're getting ready to compute the dot product\n",
    "        \n",
    "        dp = (q@k_t) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        # the dot product is possible because the last two dimensions of the two tensors is compatible.\n",
    "        \n",
    "        attn = dp.softmax(dim=-1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        # applying softmax to the last dimension to create a discrete probability distribution that sums up to 1\n",
    "        # this distribution can be used as weights in a weighted average\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg = attn @ v # (n_samples, n_heads, n_patches+!, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1,2) # (n_samples, n_patches+1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2) #(n_samples, n_patches+1, dim)\n",
    "        # the last two operations concatenate the attention heads\n",
    "        # we end up with a 3 dimensional attention head that has exactly the dimensions that we want\n",
    "        \n",
    "        x = self.proj(weighted_avg) # (n_samples, n_patches+1, dim)\n",
    "        x = self.proj_drop(x) # (n_samples, n_patches+1, dim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e853087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer\n",
    "        \n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "        \n",
    "    p : float\n",
    "        Dropout probability.\n",
    "        \n",
    "    Attribute\n",
    "    ---------\n",
    "    fc : nn.Linear\n",
    "        The first linear layer\n",
    "        \n",
    "    act : nn.GELU\n",
    "        GELU activation function\n",
    "        \n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer\n",
    "        \n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the MLP has one hidden layer\n",
    "    # we will be using the gaussian error linear unit activation function\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU\n",
    "        self.fc2 == nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            shape -> '(n_samples, n_patches + 1, in_features)'.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "           \n",
    "           shape -> '(n_samples, n_patches + 1, out_features)'\n",
    "        \"\"\"\n",
    "        \n",
    "        # similar to the attention block, we're going to be applying the linear mapping to a 3 dimensional tensor\n",
    "        x = self.fc1(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.act(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.drop(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.fc2(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.drop(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18254416",
   "metadata": {},
   "source": [
    "## putting things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc67ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for attributes we will have two normalization layers, 1 attention module and 1 MLP module\n",
    "    # we instantiate the first layer normalization and we set epsillon = 10^-6\n",
    "    # so as to match the pretrained model \n",
    "    \n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        # we define the absolute value of the hidden_features\n",
    "        self.mlp = MLP(\n",
    "                in_features=dim,\n",
    "                hidden_features=hidden_features,\n",
    "                out_features=dim,\n",
    "        )    \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        # creating a residual block so we can take the original input tensor and we add to it a new tensor\n",
    "        # this new tensor is created by applying the layer norm and the attention.\n",
    "        \n",
    "        # the second tensor is created by applying the second layer normalization and the multi-laper perceptron\n",
    "        # we are using two seperate layer norm modules and both of them will have their seperate set of parameters\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Both height and the width of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Both height and the width of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "\n",
    "    depth : int\n",
    "        Number of blocks.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP` module.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=384,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            n_classes=1000,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            n_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                in_chans=in_chans,\n",
    "                embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(n_samples, n_classes)`.\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(\n",
    "                n_samples, -1, -1\n",
    "        )  # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6232c58",
   "metadata": {},
   "source": [
    "### dropout layer extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f65f0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "p = 0.5\n",
    "\n",
    "module = torch.nn. Dropout(p)\n",
    "\n",
    "module.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b8e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.],\n",
       "        [2., 2., 0., 2., 0.],\n",
       "        [0., 0., 2., 2., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the dropout module is set to the training mode\n",
    "\n",
    "inp = torch.ones(3, 5)\n",
    "\n",
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8095174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9f582a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each forward pass will remove approx 50% of the elements and set them to 0\n",
    "# however to makeup for this removal, it will multiply the remaining elements with the constant\n",
    "# 1 / (1-p)\n",
    "1 / (1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d5968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.5, inplace=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us now set the module to evaluation mode\n",
    "module.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897f302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20e6d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that internally this training boolean got set to false\n",
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05073866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17fa58b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see, that in evaluation mode, the dropout layer behaves exactly like an identity mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d24d1",
   "metadata": {},
   "source": [
    "### linear layer behavior in case of 3 or higher dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186624c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=20, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "module = torch.nn.Linear(10, 20)\n",
    "\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933adb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 40\n",
    "# the most common way how to use the linear layer is to give it a two-dimensional input\n",
    "# first dimenion equals to the samples or the batch\n",
    "# second dimension is equal to the input features that we declared in the constructor.\n",
    "inp_2d = torch.rand(n_samples, 10)\n",
    "\n",
    "module(inp_2d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be10e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 33, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we can see for each sample, the linear layer simply took the intput features and mapped them into the output features\n",
    "# however you can use the linear layer on tensors of arbitrary dimension, higher than 2 as well\n",
    "# we only need to make sure that the input tensor's last dimension is equal to the input features that you declared in the constructor\n",
    "inp_3d = torch.rand(n_samples, 33, 10)\n",
    "\n",
    "module(inp_3d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45438107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2, 3, 4, 5, 6, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output tensor was created by applying the linear layer across all the samples and across the entire second dimenion\n",
    "\n",
    "inp_7d = torch.rand(n_samples, 2, 3, 4, 5, 6, 10)\n",
    "\n",
    "module(inp_7d).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc4e90",
   "metadata": {},
   "source": [
    "### basic properties of the layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e824a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inp = torch.tensor([[0, 4.0], [-1, 7], [3, 5]])\n",
    "# tensor with three samples and two features\n",
    "\n",
    "n_samples, n_features = inp.shape\n",
    "\n",
    "module = torch.nn.LayerNorm(n_features, elementwise_affine=False)\n",
    "# instantiates layer norm with elementwise_affine = false\n",
    "# this way there won't be any learnable parameters.\n",
    "\n",
    "sum(p.numel() for p in module.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bdbf292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 3., 4.]), tensor([2., 4., 1.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us now compute the mean and the standard devaiation for each sample of our input\n",
    "inp.mean(-1), inp.std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebaa11d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.0000, 2.0000, 2.0000], grad_fn=<MeanBackward1>),\n",
       " tensor([16.0000, 16.0000, 15.9999], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the layer norm will use these to normalize the data and will do this for each sample\n",
    "\n",
    "module(inp).mean(-1), module(inp).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a165abf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the layer norm has made sure that the mean and the standard deviation is 0 and 1 for each sample respectively\n",
    "# this process is independent for different samples\n",
    "# the batch size doesnt play any role\n",
    "\n",
    "# let us now reinstantiate the module with elementwise_affine=True\n",
    "module = torch.nn.LayerNorm(n_features, elementwise_affine=True)\n",
    "\n",
    "sum(p.numel() for p in module.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01446a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model now actually has 4 parameters that are contained in the weights and the biases of the module\n",
    "# they represent the new per feature mean and standard deviation\n",
    "# we will use that to rescale the data\n",
    "\n",
    "(module.bias, module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3968b61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000e+00, -2.9802e-08,  1.1921e-07], grad_fn=<MeanBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(inp).mean(-1), module(inp).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b92b3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 1.0000, 1.0000], grad_fn=<MeanBackward1>),\n",
       " tensor([4.0000, 4.0000, 4.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upon running the forward pass it seems that nothing changed if we caompare it to the elementwise_affin=False\n",
    "# However this time around we actually did 2 things:\n",
    "# first we used the normalization as before and then we used our learnable parameters to rescale the data\n",
    "# the parameters are initialized in a way that make it seem like the second step never happened \n",
    "# the parameters would get learned during training or we can just manually change them here\n",
    "\n",
    "module.bias.data += 1\n",
    "module.weight.data *= 4\n",
    " \n",
    "module(inp).mean(-1), module(inp).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c7fdd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3, 4, 5, 6, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after updating the parameters the forward pass returns different tensors\n",
    "# which makes it clear that the second rescaling step is actually taking place\n",
    "\n",
    "# the input tensor can have any arbitrary number of dimensions as long as the last dimesnion = no. of features\n",
    "\n",
    "module(torch.rand(n_samples, 2, 3, 4, 5, 6, n_features)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d5bafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "          [[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "           [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]]]],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is always the last dimension that is being normalized\n",
    "module(torch.rand(n_samples, 2, 3, 4, 5, 6, n_features)).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d0858fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[3.9995, 3.9996, 3.9992, 3.9981, 3.9987, 3.9981],\n",
       "            [3.9986, 3.8908, 3.9987, 3.9987, 3.9997, 3.9976],\n",
       "            [3.9660, 3.9999, 3.9999, 3.9998, 2.6700, 3.9999],\n",
       "            [3.9994, 3.9991, 3.9998, 3.9942, 3.0745, 3.9986],\n",
       "            [3.9634, 3.9995, 3.9962, 3.9907, 3.9998, 3.9994]],\n",
       "\n",
       "           [[3.9999, 3.9997, 3.9995, 3.9999, 3.9984, 3.9988],\n",
       "            [3.9998, 3.9985, 3.9996, 3.9993, 3.9994, 3.9505],\n",
       "            [3.9801, 3.9995, 3.9989, 3.9979, 3.9997, 3.9985],\n",
       "            [3.9985, 3.9990, 3.9999, 3.9989, 3.9579, 3.9657],\n",
       "            [3.9997, 3.9997, 3.9961, 3.9760, 3.9991, 3.9997]],\n",
       "\n",
       "           [[3.9938, 3.9995, 3.9801, 3.9982, 3.9955, 3.9997],\n",
       "            [3.9998, 3.9945, 3.9994, 3.9997, 3.9997, 3.9999],\n",
       "            [3.9676, 3.9998, 3.9998, 3.9998, 3.9952, 3.9998],\n",
       "            [3.9957, 3.9914, 3.9035, 3.9996, 3.9999, 3.9996],\n",
       "            [3.9560, 3.9996, 3.9990, 3.9997, 3.9937, 3.9998]],\n",
       "\n",
       "           [[3.9955, 3.9993, 3.9617, 3.9991, 3.9240, 3.9996],\n",
       "            [3.9951, 3.9998, 3.9911, 3.9999, 3.9998, 3.9994],\n",
       "            [3.9999, 3.9809, 3.9984, 3.9992, 3.9972, 3.9982],\n",
       "            [3.9831, 3.9809, 3.9974, 3.9996, 3.8360, 3.9984],\n",
       "            [3.9990, 3.9062, 3.9999, 3.9997, 3.9995, 3.9997]]],\n",
       "\n",
       "\n",
       "          [[[3.9986, 3.9972, 3.9998, 3.9978, 3.9996, 3.9997],\n",
       "            [3.9976, 3.8951, 3.9996, 3.9879, 3.9998, 3.9988],\n",
       "            [3.9988, 3.9318, 3.9995, 3.8727, 3.9991, 3.9995],\n",
       "            [3.9999, 3.9996, 3.9995, 3.9996, 3.9998, 3.9996],\n",
       "            [3.9993, 3.9984, 3.9997, 3.9994, 3.9988, 3.9998]],\n",
       "\n",
       "           [[3.9940, 3.9991, 3.9996, 3.9988, 3.9988, 3.9959],\n",
       "            [3.8239, 3.9996, 3.9999, 3.9968, 3.9994, 3.9992],\n",
       "            [3.9997, 3.9997, 3.9998, 3.9997, 3.9994, 3.9998],\n",
       "            [3.9819, 3.9996, 3.9997, 3.9993, 3.9988, 3.9997],\n",
       "            [3.9951, 3.9997, 0.4611, 3.9992, 3.9994, 3.9324]],\n",
       "\n",
       "           [[3.9998, 3.9768, 3.9985, 3.9997, 3.9908, 3.9995],\n",
       "            [3.9984, 3.9956, 3.9991, 3.9997, 3.4091, 3.9974],\n",
       "            [3.9992, 3.9998, 3.9965, 3.9984, 3.9998, 3.9924],\n",
       "            [3.9997, 3.9981, 3.9992, 3.9997, 3.9984, 3.9990],\n",
       "            [3.9997, 3.9679, 3.9901, 3.9997, 3.9948, 3.9999]],\n",
       "\n",
       "           [[3.9998, 3.9993, 3.9966, 3.9998, 3.9998, 3.9921],\n",
       "            [3.1003, 3.9966, 3.9991, 3.9998, 3.9736, 3.9986],\n",
       "            [3.9987, 3.9993, 3.9912, 3.9952, 3.9999, 3.9981],\n",
       "            [3.9398, 3.9977, 3.9996, 3.9997, 3.9996, 3.9997],\n",
       "            [3.9995, 3.9996, 3.9980, 3.9986, 3.9897, 3.9995]]],\n",
       "\n",
       "\n",
       "          [[[3.9992, 3.9986, 3.9998, 3.9990, 3.9967, 3.9988],\n",
       "            [3.9997, 3.9992, 3.9549, 3.9990, 3.9998, 3.9999],\n",
       "            [3.9999, 3.9508, 3.8624, 3.9998, 3.9998, 3.9998],\n",
       "            [3.9993, 3.9941, 3.9838, 3.9998, 3.9889, 3.9997],\n",
       "            [3.9975, 3.9999, 3.9999, 3.9826, 3.9993, 3.9986]],\n",
       "\n",
       "           [[3.9927, 3.9998, 3.9999, 3.9879, 3.9995, 3.9894],\n",
       "            [3.9700, 3.9990, 3.9950, 3.9924, 3.9997, 2.3978],\n",
       "            [3.9999, 3.9996, 3.9997, 3.9991, 3.9991, 3.9993],\n",
       "            [3.9998, 3.9831, 3.9908, 3.9182, 3.9915, 3.9998],\n",
       "            [3.9231, 3.9385, 3.9993, 3.9953, 3.9983, 3.9983]],\n",
       "\n",
       "           [[3.9997, 3.9999, 3.9998, 3.9446, 3.9997, 3.3827],\n",
       "            [3.9999, 3.9995, 3.9995, 3.9974, 3.9977, 3.9925],\n",
       "            [3.9988, 3.9999, 3.9985, 3.9921, 3.9996, 3.3927],\n",
       "            [3.9992, 3.7066, 3.9999, 3.9989, 3.9962, 3.9879],\n",
       "            [3.9987, 3.9997, 3.9998, 3.9835, 3.9780, 3.9994]],\n",
       "\n",
       "           [[3.9995, 3.9944, 3.9970, 3.9999, 3.9998, 3.9951],\n",
       "            [3.9998, 3.9998, 3.9588, 3.9996, 3.9999, 3.9983],\n",
       "            [3.9995, 3.9999, 3.9921, 3.9996, 3.9985, 3.9824],\n",
       "            [3.9998, 3.9999, 3.9977, 3.9999, 3.9999, 3.9442],\n",
       "            [3.9985, 3.9997, 3.9997, 3.9984, 3.9994, 3.9998]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[3.9990, 3.9762, 3.1963, 3.9756, 3.9911, 3.9967],\n",
       "            [3.9996, 3.9990, 3.9854, 3.9979, 3.9997, 3.9992],\n",
       "            [3.9998, 3.9991, 3.9984, 3.9991, 3.9996, 3.9990],\n",
       "            [3.9973, 3.9984, 3.9852, 3.9988, 3.8940, 3.9996],\n",
       "            [3.9997, 3.9966, 3.9566, 3.9984, 3.6196, 3.9692]],\n",
       "\n",
       "           [[3.9992, 3.9997, 3.9945, 3.9951, 3.9991, 3.9996],\n",
       "            [3.7082, 3.9997, 3.9997, 3.9888, 3.0722, 3.9997],\n",
       "            [3.9993, 3.9671, 3.9998, 3.9383, 3.9083, 3.9986],\n",
       "            [3.6558, 3.9981, 3.9994, 3.9867, 3.9994, 3.8351],\n",
       "            [3.9994, 3.9996, 3.9991, 3.9989, 3.9995, 3.9994]],\n",
       "\n",
       "           [[3.9977, 3.9992, 3.9989, 2.5408, 3.9987, 3.9955],\n",
       "            [3.9995, 3.9994, 3.9994, 3.9932, 3.9995, 3.9996],\n",
       "            [3.9991, 3.9996, 3.9998, 3.8943, 3.9975, 3.9990],\n",
       "            [3.9992, 3.9995, 3.9998, 3.9983, 3.9994, 3.9692],\n",
       "            [3.9954, 3.9998, 3.9922, 3.9596, 3.9974, 3.9985]],\n",
       "\n",
       "           [[3.9991, 3.9846, 3.9984, 3.9580, 3.4574, 3.9984],\n",
       "            [3.9980, 3.9996, 3.9988, 3.9996, 3.8584, 3.9966],\n",
       "            [3.9996, 3.9686, 3.9998, 3.9380, 3.9985, 3.9898],\n",
       "            [3.9997, 3.9998, 2.5546, 3.9996, 3.9992, 3.9994],\n",
       "            [3.9995, 3.9996, 3.9994, 3.9997, 3.9990, 3.8957]]],\n",
       "\n",
       "\n",
       "          [[[3.9999, 3.9948, 3.9983, 3.9990, 3.9987, 3.9997],\n",
       "            [3.9993, 3.9660, 3.9998, 3.9991, 3.9997, 3.9999],\n",
       "            [3.7944, 3.0569, 3.9997, 3.9472, 3.9998, 3.9995],\n",
       "            [3.9978, 3.9908, 3.9983, 3.9990, 3.9988, 3.9997],\n",
       "            [3.9993, 3.9998, 3.9990, 3.9925, 3.9993, 3.9991]],\n",
       "\n",
       "           [[3.9976, 3.9961, 3.9931, 3.9997, 3.9991, 3.9954],\n",
       "            [3.9948, 3.2945, 3.9997, 3.9997, 3.5393, 3.9986],\n",
       "            [3.9827, 3.9558, 3.9997, 3.9998, 3.9993, 3.9993],\n",
       "            [3.9873, 3.9999, 3.9983, 3.9993, 3.9994, 3.9992],\n",
       "            [3.9994, 3.9942, 3.9993, 3.9999, 3.9998, 3.9998]],\n",
       "\n",
       "           [[3.9898, 3.9999, 3.9988, 3.9998, 3.9995, 3.9994],\n",
       "            [3.9983, 3.9944, 3.9999, 3.9995, 3.9998, 3.9982],\n",
       "            [3.9988, 3.9999, 3.9998, 3.9770, 3.9996, 3.9992],\n",
       "            [3.9998, 3.9594, 3.9982, 3.9993, 3.9985, 3.9998],\n",
       "            [3.9988, 3.9998, 3.9973, 3.9998, 3.7711, 3.9993]],\n",
       "\n",
       "           [[3.9999, 3.9987, 3.9974, 3.9813, 3.9998, 3.9996],\n",
       "            [3.9979, 3.9983, 3.9984, 3.9998, 3.8634, 3.8806],\n",
       "            [3.9995, 3.8996, 3.9987, 3.9982, 3.9998, 3.0635],\n",
       "            [3.9986, 3.9993, 3.9975, 3.9985, 3.9969, 3.9998],\n",
       "            [3.9998, 3.9993, 3.9971, 3.9991, 3.9942, 3.9991]]],\n",
       "\n",
       "\n",
       "          [[[3.9968, 3.9991, 3.9949, 3.9996, 3.9957, 3.9993],\n",
       "            [3.9978, 3.9997, 3.9985, 3.9560, 3.9986, 3.9997],\n",
       "            [3.9987, 3.9986, 3.9997, 3.9884, 3.9916, 3.9995],\n",
       "            [3.9997, 3.9993, 3.9984, 3.9957, 3.9880, 3.9997],\n",
       "            [3.9984, 3.9997, 3.9954, 3.9996, 3.9879, 3.9992]],\n",
       "\n",
       "           [[3.9941, 3.9999, 3.9255, 3.9574, 3.9976, 3.9998],\n",
       "            [3.9998, 3.9963, 3.9999, 3.9997, 3.9933, 3.9976],\n",
       "            [3.9997, 3.9998, 3.9997, 3.9997, 3.9984, 3.9590],\n",
       "            [3.9993, 3.9999, 3.9881, 3.9998, 3.9990, 3.9985],\n",
       "            [3.9956, 3.9998, 3.9999, 3.9931, 3.9988, 3.9999]],\n",
       "\n",
       "           [[3.9998, 3.9993, 3.9999, 3.9988, 3.9458, 3.9999],\n",
       "            [3.9994, 3.9998, 3.9999, 3.9914, 3.9998, 3.9997],\n",
       "            [3.9997, 3.9995, 3.9817, 3.9998, 3.8400, 3.9996],\n",
       "            [3.9995, 3.9985, 1.4334, 3.9670, 3.9974, 3.9987],\n",
       "            [3.8526, 3.9996, 3.8874, 3.9982, 3.9700, 3.9948]],\n",
       "\n",
       "           [[3.9993, 3.9992, 3.9883, 3.9942, 3.9998, 3.9987],\n",
       "            [3.9991, 3.9851, 3.9871, 3.9758, 3.9996, 3.9987],\n",
       "            [3.9986, 3.9937, 3.9998, 3.9925, 3.9991, 3.9997],\n",
       "            [2.6322, 3.9991, 3.9779, 3.9853, 3.9989, 3.9994],\n",
       "            [3.9989, 3.9944, 3.9995, 3.5578, 3.9998, 3.9900]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[3.9989, 3.9868, 3.9994, 3.9992, 3.9997, 3.9984],\n",
       "            [3.9823, 3.9909, 3.9827, 3.9996, 3.9982, 3.9881],\n",
       "            [3.9982, 3.9997, 3.9997, 3.9999, 3.9995, 3.9994],\n",
       "            [3.9938, 3.9982, 3.9988, 3.9998, 3.9986, 3.9985],\n",
       "            [3.9979, 3.5769, 3.9995, 3.8127, 3.9919, 3.9976]],\n",
       "\n",
       "           [[3.9993, 2.3077, 3.9994, 3.9998, 3.9842, 3.9996],\n",
       "            [3.9998, 3.9998, 3.9033, 3.9991, 3.9994, 3.9997],\n",
       "            [3.9998, 3.9763, 3.9994, 3.9998, 3.9998, 3.9997],\n",
       "            [3.9972, 3.9997, 3.9992, 3.9997, 3.9999, 3.9998],\n",
       "            [3.9991, 3.8248, 3.9988, 3.9998, 3.9909, 3.9991]],\n",
       "\n",
       "           [[3.9997, 3.9997, 3.9984, 3.9997, 3.9971, 3.9982],\n",
       "            [3.9986, 3.9998, 3.9997, 0.2268, 3.9943, 3.9983],\n",
       "            [3.9975, 3.9987, 3.9963, 3.9987, 3.9999, 3.9999],\n",
       "            [3.9994, 1.5314, 3.9800, 3.9989, 3.9955, 3.9901],\n",
       "            [3.9993, 3.9998, 3.9998, 3.9996, 3.8515, 3.9992]],\n",
       "\n",
       "           [[3.9997, 3.9998, 3.9970, 3.9996, 3.9975, 3.9992],\n",
       "            [3.9998, 3.9917, 3.9991, 3.9991, 3.9998, 3.9994],\n",
       "            [3.9998, 3.9999, 3.9919, 3.9998, 3.9899, 3.9994],\n",
       "            [3.9997, 3.9974, 3.9995, 3.9895, 3.9999, 3.9994],\n",
       "            [3.9247, 3.9667, 3.9990, 3.9107, 3.9982, 3.9971]]],\n",
       "\n",
       "\n",
       "          [[[3.9995, 3.9998, 3.9999, 3.9960, 3.9895, 3.9993],\n",
       "            [3.9998, 3.9996, 3.9688, 3.9998, 3.9517, 3.6036],\n",
       "            [3.9998, 3.9778, 3.9999, 3.9997, 3.9999, 3.9998],\n",
       "            [3.9997, 3.9981, 3.9985, 3.9993, 3.2883, 3.9993],\n",
       "            [3.9758, 3.9999, 3.9964, 3.9998, 3.9995, 3.9991]],\n",
       "\n",
       "           [[3.9974, 3.9979, 3.9998, 3.9984, 3.9995, 3.9990],\n",
       "            [3.9997, 3.9995, 3.9851, 3.9999, 3.9993, 3.9988],\n",
       "            [3.9993, 3.9983, 3.9942, 3.8937, 3.6871, 3.9905],\n",
       "            [3.9888, 3.9982, 3.9997, 3.9994, 3.9995, 3.9983],\n",
       "            [3.9846, 3.9996, 3.9997, 3.9998, 3.9967, 3.9113]],\n",
       "\n",
       "           [[3.9989, 3.9964, 3.9988, 3.9996, 3.9958, 3.9991],\n",
       "            [3.9996, 3.9996, 3.9865, 3.9994, 3.9999, 3.9972],\n",
       "            [3.9959, 3.9989, 3.9946, 3.9997, 3.9998, 3.9998],\n",
       "            [3.9998, 3.9999, 3.9995, 3.9985, 3.9758, 3.9997],\n",
       "            [3.9990, 3.9998, 3.9998, 3.9903, 3.9998, 3.9795]],\n",
       "\n",
       "           [[3.9998, 3.7409, 3.9966, 3.9998, 3.9888, 3.9896],\n",
       "            [3.9363, 3.9981, 3.9982, 3.9972, 3.9983, 3.9998],\n",
       "            [3.9993, 3.9987, 3.9515, 3.9986, 3.9992, 3.9496],\n",
       "            [3.9994, 3.9998, 3.9837, 3.9997, 3.9967, 3.9996],\n",
       "            [3.9993, 3.9994, 3.9990, 3.9332, 3.9995, 3.9999]]],\n",
       "\n",
       "\n",
       "          [[[3.9990, 3.9992, 3.9981, 3.9995, 3.9998, 3.9997],\n",
       "            [3.9978, 3.9924, 3.9996, 3.9996, 3.9988, 3.9998],\n",
       "            [3.9950, 3.2998, 3.9997, 3.9977, 3.9995, 3.9998],\n",
       "            [3.9989, 3.9994, 0.9298, 3.9998, 3.9998, 3.9983],\n",
       "            [3.9921, 3.9999, 3.4825, 3.9997, 3.9341, 3.9948]],\n",
       "\n",
       "           [[3.9995, 3.9998, 3.9992, 3.9989, 3.9998, 3.9998],\n",
       "            [3.9968, 3.9565, 3.9996, 3.9996, 3.9632, 3.9987],\n",
       "            [2.3029, 3.9992, 3.9997, 3.9455, 3.9998, 3.9999],\n",
       "            [3.9971, 1.2648, 3.9985, 3.9871, 3.9996, 3.9968],\n",
       "            [3.9982, 3.9999, 3.9994, 3.9997, 3.9988, 3.9979]],\n",
       "\n",
       "           [[3.9593, 3.9999, 3.9997, 3.9980, 3.9990, 3.9516],\n",
       "            [3.9910, 3.9997, 3.9873, 3.9992, 3.9998, 3.9965],\n",
       "            [2.3698, 3.9634, 3.9772, 3.9999, 3.9973, 3.9702],\n",
       "            [3.9981, 1.1034, 3.9982, 3.8565, 3.9920, 3.9999],\n",
       "            [3.9992, 3.9874, 3.9995, 3.9791, 3.9998, 3.9999]],\n",
       "\n",
       "           [[3.9997, 3.9997, 3.9457, 3.9996, 3.9979, 3.9993],\n",
       "            [3.9998, 1.0982, 3.9972, 3.9849, 3.9993, 3.9995],\n",
       "            [0.3001, 3.9997, 3.9838, 3.9996, 3.9947, 3.9999],\n",
       "            [3.9991, 3.9999, 3.9999, 3.9994, 3.9995, 3.9997],\n",
       "            [3.9961, 3.9998, 3.9633, 3.9994, 3.9982, 3.9998]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[3.9933, 3.9998, 3.9999, 3.9708, 3.9780, 3.9998],\n",
       "            [3.9999, 3.9997, 3.9998, 3.9995, 3.9964, 2.2800],\n",
       "            [3.9987, 3.9999, 3.9998, 3.9998, 3.9810, 3.9907],\n",
       "            [3.9996, 3.9993, 3.9989, 3.9990, 3.9483, 3.9898],\n",
       "            [3.9987, 3.9965, 3.9998, 3.9993, 3.9899, 3.9994]],\n",
       "\n",
       "           [[3.8745, 3.9991, 3.9997, 3.9992, 3.9902, 3.9992],\n",
       "            [3.9793, 3.9997, 3.9739, 3.9998, 3.9996, 3.6697],\n",
       "            [3.9995, 3.9997, 3.9999, 3.9683, 3.9759, 3.9987],\n",
       "            [3.9986, 3.9998, 3.8918, 3.8270, 3.9921, 3.9912],\n",
       "            [3.9997, 3.9998, 3.9975, 3.9999, 3.9952, 3.9972]],\n",
       "\n",
       "           [[3.9996, 3.9997, 3.9990, 3.9997, 3.9995, 3.9996],\n",
       "            [3.9997, 3.9993, 3.9851, 3.9972, 3.9142, 3.9997],\n",
       "            [3.9997, 3.9998, 3.9997, 3.9988, 3.9997, 3.9696],\n",
       "            [3.9997, 3.9988, 3.9936, 3.9884, 3.9974, 3.9992],\n",
       "            [3.9985, 3.9967, 3.9999, 3.9786, 3.9989, 3.9124]],\n",
       "\n",
       "           [[3.9993, 3.9990, 3.9998, 3.9772, 3.9966, 3.9994],\n",
       "            [3.9995, 3.4773, 3.9952, 3.9966, 3.9958, 3.9996],\n",
       "            [3.9991, 3.9974, 3.9991, 3.9985, 3.9998, 3.9998],\n",
       "            [3.9990, 3.9998, 3.9994, 3.9988, 3.9957, 3.7402],\n",
       "            [3.9998, 3.9986, 3.9939, 3.9955, 3.9996, 3.9994]]],\n",
       "\n",
       "\n",
       "          [[[3.9993, 3.9994, 3.9994, 3.9999, 3.9992, 3.9970],\n",
       "            [3.7589, 3.1317, 3.9990, 3.8620, 3.9993, 3.9966],\n",
       "            [3.9993, 3.9989, 3.9998, 3.9999, 3.9990, 3.9999],\n",
       "            [3.9989, 3.9994, 3.9980, 3.9997, 3.9976, 3.9996],\n",
       "            [3.9998, 3.9996, 3.9993, 3.9987, 3.9992, 3.9991]],\n",
       "\n",
       "           [[3.9998, 3.9993, 3.9996, 3.9987, 3.9830, 3.9597],\n",
       "            [3.9959, 3.9996, 3.9998, 3.9822, 3.9993, 3.9997],\n",
       "            [3.9997, 3.9974, 3.9983, 3.9995, 3.9817, 3.9897],\n",
       "            [3.9960, 3.9986, 3.9996, 3.9998, 3.9995, 3.9998],\n",
       "            [3.5207, 3.9998, 3.9486, 3.9997, 3.9990, 3.9907]],\n",
       "\n",
       "           [[3.9994, 3.9994, 3.9992, 3.9973, 3.9980, 3.9985],\n",
       "            [3.9999, 3.9941, 3.9999, 3.9987, 3.9995, 3.9997],\n",
       "            [3.9864, 3.9818, 3.9999, 3.9998, 3.9997, 3.9995],\n",
       "            [3.9772, 3.9994, 3.9999, 3.9728, 3.9997, 3.9981],\n",
       "            [3.9993, 3.9999, 3.9995, 3.9989, 3.9938, 3.9955]],\n",
       "\n",
       "           [[3.9997, 3.9991, 3.9996, 3.9995, 3.9994, 3.9956],\n",
       "            [3.9977, 3.9978, 3.9989, 3.9994, 3.9924, 3.9979],\n",
       "            [3.9998, 3.9985, 3.9985, 3.9996, 3.9839, 3.9755],\n",
       "            [3.9937, 3.9972, 3.9957, 3.9998, 3.9992, 3.9994],\n",
       "            [3.9986, 3.9999, 3.9970, 3.7916, 3.9998, 3.9941]]],\n",
       "\n",
       "\n",
       "          [[[3.9998, 3.9998, 3.9992, 3.5014, 3.9988, 3.9995],\n",
       "            [3.9994, 3.9976, 3.9999, 3.9993, 3.9976, 3.9998],\n",
       "            [3.9999, 3.9145, 3.9998, 3.9833, 3.9707, 3.9993],\n",
       "            [3.9992, 3.9975, 3.9988, 3.9999, 3.9992, 3.9862],\n",
       "            [3.9948, 3.9976, 3.9131, 3.9997, 3.9994, 3.9998]],\n",
       "\n",
       "           [[3.9997, 3.8555, 3.9998, 3.9999, 3.9958, 3.9997],\n",
       "            [3.9995, 3.9994, 3.9112, 3.9998, 3.9852, 3.9871],\n",
       "            [3.9987, 3.9993, 3.9999, 3.9982, 2.2433, 3.9998],\n",
       "            [3.9945, 3.9997, 3.9966, 3.9889, 3.9994, 3.9750],\n",
       "            [3.9996, 3.9967, 3.9974, 3.9725, 3.9995, 3.9993]],\n",
       "\n",
       "           [[3.9999, 3.9985, 3.9996, 3.9874, 3.9923, 3.9983],\n",
       "            [3.9589, 3.9990, 3.9996, 3.9792, 3.9997, 3.9727],\n",
       "            [3.9990, 3.9999, 3.9996, 3.9997, 3.9953, 3.9970],\n",
       "            [3.9993, 3.9944, 3.0825, 3.9991, 3.9999, 3.9998],\n",
       "            [3.9968, 3.9994, 3.9623, 3.9964, 3.9998, 3.9987]],\n",
       "\n",
       "           [[3.9962, 3.9993, 3.9971, 3.9984, 3.9996, 3.9995],\n",
       "            [1.0639, 3.9990, 3.9998, 3.9912, 3.9897, 3.9915],\n",
       "            [3.9986, 3.9997, 3.9998, 3.9990, 3.9996, 3.9943],\n",
       "            [3.9997, 3.9998, 3.9998, 3.9965, 3.9979, 3.9996],\n",
       "            [3.9991, 3.9999, 3.9991, 3.9998, 3.9983, 3.9992]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[3.9998, 3.9997, 3.9950, 3.9989, 3.9991, 3.9970],\n",
       "            [3.9998, 3.9995, 3.9986, 3.9504, 3.9997, 3.9997],\n",
       "            [3.9988, 3.9997, 3.9994, 3.9998, 3.9994, 3.9993],\n",
       "            [3.9998, 3.9998, 3.6504, 3.9972, 3.8367, 3.9999],\n",
       "            [3.9986, 3.9986, 3.9900, 3.9998, 3.9968, 3.9988]],\n",
       "\n",
       "           [[3.9998, 3.9996, 3.9996, 3.9953, 3.9998, 3.9984],\n",
       "            [3.9990, 3.9891, 3.9979, 3.9975, 3.9989, 3.9963],\n",
       "            [3.9998, 3.9929, 3.9905, 3.9916, 3.9995, 3.9986],\n",
       "            [3.9999, 3.9998, 3.9933, 3.9991, 3.9981, 3.9996],\n",
       "            [3.9922, 3.5402, 3.9996, 3.9995, 3.9947, 3.9978]],\n",
       "\n",
       "           [[3.9983, 3.9998, 3.9901, 3.9998, 3.9993, 3.9997],\n",
       "            [3.9998, 3.9957, 1.2112, 3.9994, 3.9944, 3.9994],\n",
       "            [3.9681, 3.7401, 3.9996, 3.8987, 3.9994, 3.9998],\n",
       "            [3.8077, 3.9996, 3.9996, 3.9999, 3.9996, 2.8550],\n",
       "            [3.9955, 3.9981, 3.9898, 3.9985, 3.9997, 3.9995]],\n",
       "\n",
       "           [[3.9966, 3.9973, 3.9990, 3.9998, 3.9996, 3.9993],\n",
       "            [3.9999, 3.9983, 3.9998, 3.9994, 3.9998, 3.9992],\n",
       "            [3.9996, 3.9992, 3.9996, 3.9912, 3.9997, 3.9929],\n",
       "            [3.9995, 3.9891, 3.9999, 3.9991, 3.9997, 3.9993],\n",
       "            [3.9963, 3.9991, 3.9998, 3.9989, 3.9997, 3.9880]]],\n",
       "\n",
       "\n",
       "          [[[3.9997, 3.9957, 3.7646, 3.9998, 3.9997, 3.9994],\n",
       "            [3.9999, 3.9996, 3.9963, 3.9993, 3.9991, 3.9997],\n",
       "            [3.9998, 3.9998, 3.9993, 3.9997, 3.9997, 3.9997],\n",
       "            [3.9936, 3.9996, 3.9993, 3.9996, 3.9998, 3.9995],\n",
       "            [3.9979, 3.9998, 3.9997, 3.9999, 3.9884, 3.9994]],\n",
       "\n",
       "           [[0.3600, 3.9969, 3.7089, 3.9999, 3.9996, 3.9993],\n",
       "            [3.9993, 3.9998, 3.9984, 3.9998, 3.9959, 3.9996],\n",
       "            [0.1696, 1.6261, 3.6998, 3.9997, 3.9969, 1.2748],\n",
       "            [3.9995, 3.9993, 3.9998, 3.9996, 3.9945, 3.9997],\n",
       "            [3.9566, 3.9979, 3.9994, 3.9997, 3.9951, 3.9994]],\n",
       "\n",
       "           [[3.9994, 3.9986, 3.9962, 3.9998, 3.9965, 3.9987],\n",
       "            [3.9919, 3.9977, 3.9996, 3.9999, 3.9999, 3.9918],\n",
       "            [3.5458, 3.9964, 3.7615, 3.9994, 3.9996, 3.9991],\n",
       "            [3.9932, 3.9429, 3.9995, 3.9998, 3.9997, 3.9998],\n",
       "            [3.9701, 3.9997, 3.9987, 3.9986, 3.8108, 3.9998]],\n",
       "\n",
       "           [[3.9946, 3.9996, 3.9997, 3.9993, 3.9997, 3.9997],\n",
       "            [3.9935, 3.9973, 3.9997, 3.9980, 3.9996, 3.9960],\n",
       "            [3.9981, 3.9890, 3.9983, 3.9990, 3.9992, 3.9952],\n",
       "            [3.9995, 3.9926, 3.9996, 3.9994, 3.9981, 3.9997],\n",
       "            [3.9986, 3.7955, 3.9991, 3.9982, 3.9991, 3.9975]]],\n",
       "\n",
       "\n",
       "          [[[3.9771, 3.9951, 3.9995, 3.9997, 3.9985, 3.9981],\n",
       "            [3.9998, 3.9647, 3.9994, 3.9881, 3.9993, 2.8571],\n",
       "            [3.9996, 3.9168, 3.9992, 3.9992, 3.9989, 3.9998],\n",
       "            [3.9990, 3.9987, 3.9917, 3.9991, 3.9999, 3.9977],\n",
       "            [3.9834, 3.9999, 3.9986, 3.9990, 3.9992, 3.9997]],\n",
       "\n",
       "           [[3.9985, 3.3011, 3.9981, 3.9996, 3.9998, 3.9995],\n",
       "            [3.9995, 3.9997, 3.9988, 3.9998, 3.8658, 3.9993],\n",
       "            [3.9998, 3.9990, 3.9977, 3.9979, 3.9998, 3.9999],\n",
       "            [3.9889, 3.9992, 3.9997, 3.9953, 3.9994, 3.9999],\n",
       "            [3.9999, 3.9951, 3.9996, 3.9995, 3.9957, 3.9955]],\n",
       "\n",
       "           [[3.9974, 3.9989, 3.9996, 3.9981, 3.9412, 3.9997],\n",
       "            [3.9286, 3.9997, 3.9995, 3.9996, 3.9996, 3.9971],\n",
       "            [3.9838, 3.9780, 3.9955, 3.9997, 3.9999, 3.9989],\n",
       "            [3.9992, 3.9988, 3.9965, 3.9995, 3.9993, 3.9972],\n",
       "            [3.9986, 3.9938, 3.9998, 3.9972, 3.9964, 3.9995]],\n",
       "\n",
       "           [[3.9995, 3.9998, 3.9994, 3.9966, 3.9989, 3.9992],\n",
       "            [3.9995, 3.9997, 3.9998, 3.6420, 3.9796, 3.9988],\n",
       "            [3.9998, 3.9690, 3.9989, 3.9985, 3.9996, 3.9985],\n",
       "            [3.9963, 3.9995, 3.9996, 3.8927, 3.9995, 3.9996],\n",
       "            [3.9998, 3.9997, 3.9991, 3.9965, 3.9993, 3.9986]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[3.9996, 3.9945, 3.9975, 3.9990, 3.9550, 3.8966],\n",
       "            [3.9994, 3.9994, 3.8639, 3.9982, 3.9979, 3.9748],\n",
       "            [3.9994, 3.9994, 3.9751, 3.9998, 3.9988, 3.9986],\n",
       "            [3.9999, 3.9998, 3.9988, 3.9957, 3.9970, 3.9998],\n",
       "            [3.9882, 3.9998, 3.9997, 3.9998, 3.9989, 3.9995]],\n",
       "\n",
       "           [[3.9976, 3.9949, 3.9994, 3.9997, 3.9995, 3.9999],\n",
       "            [3.9983, 3.9886, 3.5941, 3.8873, 3.9979, 3.9998],\n",
       "            [3.9995, 3.9997, 3.9997, 3.9999, 3.9998, 3.9981],\n",
       "            [3.9784, 3.9999, 3.9995, 3.9560, 3.9988, 3.9998],\n",
       "            [3.9993, 3.9996, 3.9992, 3.9998, 3.5483, 3.9997]],\n",
       "\n",
       "           [[3.9998, 3.9969, 3.9996, 3.9999, 3.9985, 3.9997],\n",
       "            [3.9975, 3.9996, 3.9971, 3.9998, 3.9862, 3.9962],\n",
       "            [3.9977, 3.9982, 3.9813, 3.8901, 3.9985, 3.9998],\n",
       "            [3.9996, 3.9994, 3.9302, 3.9997, 3.9994, 3.9997],\n",
       "            [3.9998, 3.9763, 3.9893, 3.9992, 3.9992, 3.9993]],\n",
       "\n",
       "           [[3.9907, 3.9996, 3.9996, 3.9996, 3.9979, 3.9980],\n",
       "            [3.9996, 3.9955, 3.9997, 3.9991, 3.9996, 3.9972],\n",
       "            [3.9985, 3.9997, 3.9994, 3.9982, 3.9996, 3.9993],\n",
       "            [3.9897, 3.9988, 3.9935, 3.9996, 3.9996, 3.9994],\n",
       "            [3.7841, 3.9991, 3.9999, 3.9968, 3.9955, 3.9996]]],\n",
       "\n",
       "\n",
       "          [[[3.9953, 3.9443, 3.9964, 3.9539, 3.9800, 3.9995],\n",
       "            [3.9953, 3.9309, 3.9998, 3.9996, 3.9996, 3.9986],\n",
       "            [3.9996, 3.5061, 3.9921, 3.9994, 3.9813, 3.9994],\n",
       "            [3.9983, 3.4708, 3.9995, 3.9952, 3.9998, 3.9997],\n",
       "            [3.9980, 3.9997, 3.9997, 3.9968, 3.3242, 3.9999]],\n",
       "\n",
       "           [[3.9979, 3.9996, 3.9612, 3.9999, 3.9991, 3.9992],\n",
       "            [3.9997, 3.9783, 3.9989, 3.9987, 3.9999, 3.9992],\n",
       "            [3.9998, 3.9970, 3.9995, 3.9950, 3.7897, 3.9994],\n",
       "            [3.9999, 3.9995, 3.9963, 3.9977, 3.9997, 3.9995],\n",
       "            [3.9807, 3.9889, 3.9999, 3.9985, 3.9995, 3.9995]],\n",
       "\n",
       "           [[3.9997, 3.9986, 3.9924, 3.9720, 3.9996, 3.9989],\n",
       "            [3.9995, 3.9396, 3.9996, 3.9997, 3.9997, 3.9959],\n",
       "            [3.9974, 3.9988, 3.9995, 3.9995, 3.9990, 3.9994],\n",
       "            [3.9989, 3.9991, 3.9992, 3.9969, 3.9905, 3.9998],\n",
       "            [3.9874, 3.9998, 3.9952, 3.9994, 3.9992, 3.9990]],\n",
       "\n",
       "           [[3.9996, 3.9950, 3.9998, 3.9980, 3.9964, 3.9995],\n",
       "            [3.9993, 3.9992, 3.9993, 3.9969, 3.9988, 3.9995],\n",
       "            [3.9988, 3.9995, 3.9989, 3.9970, 3.9997, 3.9992],\n",
       "            [0.4264, 3.9699, 3.9998, 3.9992, 3.9996, 3.9999],\n",
       "            [3.9991, 3.9993, 3.9438, 3.9939, 3.9992, 3.9999]]],\n",
       "\n",
       "\n",
       "          [[[3.9996, 3.9997, 3.9981, 3.9998, 3.9992, 3.9946],\n",
       "            [3.9698, 3.9751, 3.9970, 3.9965, 3.9999, 3.9998],\n",
       "            [3.9995, 2.7365, 3.9998, 3.9997, 3.9863, 3.6997],\n",
       "            [3.9590, 3.8989, 3.9990, 3.9999, 3.9984, 3.9992],\n",
       "            [3.9941, 3.9998, 3.9712, 3.9998, 3.9990, 3.9991]],\n",
       "\n",
       "           [[3.9999, 3.9961, 3.9995, 3.9997, 3.9967, 3.9998],\n",
       "            [3.6618, 3.9995, 3.9998, 3.9987, 3.9998, 3.9998],\n",
       "            [3.9947, 3.9999, 3.9998, 3.9993, 3.9996, 3.9893],\n",
       "            [3.9992, 3.9875, 3.9987, 3.9997, 3.9815, 3.9616],\n",
       "            [3.9993, 3.9991, 3.9965, 3.9996, 3.9993, 3.9999]],\n",
       "\n",
       "           [[3.9980, 3.7857, 3.9982, 3.8880, 3.9999, 3.9998],\n",
       "            [3.9998, 3.9997, 2.8630, 3.9972, 3.9989, 3.9993],\n",
       "            [3.9997, 3.9997, 3.9998, 3.8877, 3.9998, 3.9983],\n",
       "            [3.9998, 3.9988, 1.8290, 3.8892, 3.9995, 3.9996],\n",
       "            [3.9926, 3.9937, 3.9542, 3.9994, 3.9977, 3.9520]],\n",
       "\n",
       "           [[3.9999, 3.9963, 3.9045, 3.9971, 3.9996, 3.9999],\n",
       "            [2.8419, 3.9960, 3.9997, 3.9813, 3.9998, 3.9868],\n",
       "            [3.9208, 3.9997, 3.9743, 3.9973, 3.9993, 3.9980],\n",
       "            [3.9970, 3.9995, 3.9998, 3.9999, 3.9994, 3.9993],\n",
       "            [3.9986, 3.9986, 3.9996, 3.9992, 3.9998, 3.9897]]]]]],\n",
       "       grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(torch.rand(n_samples, 2, 3, 4, 5, 6, n_features)).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f6ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
